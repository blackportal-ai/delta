// BSD 3-Clause License
//
// Copyright (c) 2025, BlackPortal â—‹
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are met:
//
// 1. Redistributions of source code must retain the above copyright notice, this
//    list of conditions and the following disclaimer.
//
// 2. Redistributions in binary form must reproduce the above copyright notice,
//    this list of conditions and the following disclaimer in the documentation
//    and/or other materials provided with the distribution.
//
// 3. Neither the name of the copyright holder nor the names of its
//    contributors may be used to endorse or promote products derived from
//    this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

use ndarray::{Dimension, IxDyn, Shape};

use crate::devices::Device;

use std::fmt::Debug;

use super::{tensor_ops::Tensor, utils::check_for_nan};

/// A trait representing a loss function.
pub trait Loss: Debug {
    /// Calculates the loss between the output and the target tensors.
    ///
    /// # Arguments
    ///
    /// * `output` - The output tensor from the model.
    /// * `target` - The target tensor.
    ///
    /// # Returns
    ///
    /// The calculated loss as a `f32` value.
    fn calculate_loss(&self, output: &Tensor, target: &Tensor) -> f32;

    /// Calculates the gradient of the loss with respect to the output tensor.
    ///
    /// # Arguments
    ///
    /// * `output` - The output tensor from the model.
    /// * `target` - The target tensor.
    ///
    /// # Returns
    ///
    /// A `Tensor` containing the gradient of the loss with respect to the output tensor.
    fn calculate_loss_grad(&self, output: &Tensor, target: &Tensor) -> Tensor;
}

/// A struct representing the Cross-Entropy Loss function.
#[derive(Debug)]
pub struct CrossEntropyLoss;

impl Default for CrossEntropyLoss {
    fn default() -> Self {
        Self::new()
    }
}

impl CrossEntropyLoss {
    /// Creates a new CrossEntropyLoss instance.
    pub fn new() -> Self {
        Self
    }

    /// Clips the tensor to avoid issues with log(0) or division by zero.
    ///
    /// # Arguments
    ///
    /// * `tensor` - The tensor to clip.
    /// * `epsilon` - The epsilon value to use for clipping.
    ///
    /// # Returns
    ///
    /// The clipped tensor.
    fn clip_tensor(&self, tensor: &Tensor, epsilon: f32) -> Tensor {
        tensor.map(|x| x.clamp(epsilon, 1.0 - epsilon))
    }
}

impl Loss for CrossEntropyLoss {
    /// Calculates the cross-entropy loss between the true and predicted values.
    ///
    /// # Arguments
    ///
    /// * `y_true` - The true values.
    /// * `y_pred` - The predicted values.
    ///
    /// # Returns
    ///
    /// The cross-entropy loss.
    fn calculate_loss(&self, y_true: &Tensor, y_pred: &Tensor) -> f32 {
        if y_true.shape().raw_dim() != y_pred.shape().raw_dim() {
            panic!(
                "Shape mismatch: y_true.shape = {:?}, y_pred.shape = {:?}",
                y_true.shape().raw_dim(),
                y_pred.shape().raw_dim()
            );
        }

        // Check for NaN values in y_true and y_pred
        check_for_nan(y_true, y_pred);

        // Clip predictions to avoid log(0)
        let epsilon = 1e-12;
        let clipped_pred = self.clip_tensor(y_pred, epsilon);

        // Compute element-wise multiplication
        let cross_entropy =
            y_true.data.iter().zip(clipped_pred.data.iter()).map(|(t, p)| t * p.ln()).sum::<f32>();

        // Return cross-entropy loss
        -cross_entropy
    }

    /// Calculates the gradient of the cross-entropy loss with respect to the predicted values.
    ///
    /// # Arguments
    ///
    /// * `output` - The predicted values.
    /// * `target` - The true values.
    ///
    /// # Returns
    ///
    /// The gradient of the cross-entropy loss with respect to the predicted values.
    fn calculate_loss_grad(&self, output: &Tensor, target: &Tensor) -> Tensor {
        // Ensure shapes match
        if output.shape().raw_dim() != target.shape().raw_dim() {
            panic!(
                "Shape mismatch: output.shape = {:?}, target.shape = {:?}",
                output.shape(),
                target.shape()
            );
        }

        // Clip predictions to avoid division by zero
        let epsilon = 1e-12;
        let clipped_pred = self.clip_tensor(output, epsilon);

        // Compute gradient: (predictions - targets) / batch_size
        let grad_data: Vec<f32> = clipped_pred
            .data
            .iter()
            .zip(target.data.iter())
            .map(|(p, t)| (p - t) / target.data.len() as f32)
            .collect();

        // Create Tensor from grad_data
        let grad_shape = clipped_pred.shape().raw_dim().as_array_view().to_vec(); // Ensure correct shape
        Tensor {
            data: ndarray::Array::from_shape_vec(ndarray::IxDyn(&grad_shape), grad_data)
                .expect("Failed to create gradient tensor"),
            device: Device::default(),
        }
    }
}

#[derive(Debug)]
pub struct HuberLoss {
    delta: f32,
}

impl HuberLoss {
    /// Creates a new HuberLoss instance with the specified delta value.
    ///
    /// # Arguments
    ///
    /// * `delta` - The delta value for the Huber loss.
    pub fn new(delta: f32) -> Self {
        if delta <= 0.0 {
            panic!("Delta must be positive.");
        }
        Self { delta }
    }
}

impl Loss for HuberLoss {
    /// Calculates the Huber loss between two tensors.
    ///
    /// # Arguments
    /// * `y_true` - The ground truth tensor.
    /// * `y_pred` - The predicted tensor.
    ///
    /// # Returns
    ///
    /// The Huber loss between the two tensors.
    fn calculate_loss(&self, y_true: &Tensor, y_pred: &Tensor) -> f32 {
        // Step 1: Ensure the shapes of y_true and y_pred match
        if y_true.data.shape() != y_pred.data.shape() {
            panic!(
                "Shape mismatch: y_true.shape = {:?}, y_pred.shape = {:?}",
                y_true.data.shape(),
                y_pred.data.shape()
            );
        }

        // Step 2: Check for NaN values in y_true and y_pred
        check_for_nan(y_true, y_pred);

        // Step 3: Compute the absolute differences
        let diff = (&y_true.data - &y_pred.data).mapv(|x| x.abs());

        // Step 4: Compute the Huber loss per element
        let huber_loss = diff.mapv(|x| {
            if x <= self.delta { 0.5 * x.powi(2) } else { self.delta * (x - 0.5 * self.delta) }
        });

        // Step 5: Calculate the mean of the Huber loss values
        if huber_loss.is_empty() {
            panic!("Cannot calculate loss: no dataset in input tensors");
        }

        huber_loss.mean().expect("Mean computation failed unexpectedly")
    }

    /// Calculates the gradient of the Huber loss with respect to the output tensor.
    ///
    /// # Arguments
    /// * `output` - The output tensor from the model.
    /// * `target` - The target tensor.
    ///
    /// # Returns
    ///
    /// A `Tensor` containing the gradient of the Huber loss with respect to the output tensor.
    fn calculate_loss_grad(&self, output: &Tensor, target: &Tensor) -> Tensor {
        // Ensure shapes match
        if output.data.shape() != target.data.shape() {
            panic!(
                "Shape mismatch: output.shape = {:?}, target.shape = {:?}",
                output.data.shape(),
                target.data.shape()
            );
        }

        // Calculate the difference
        let diff = &output.data - &target.data;

        // Compute the gradient
        let gradient =
            diff.mapv(|x| if x.abs() <= self.delta { x } else { self.delta * x.signum() });

        // Normalize the gradient by the number of elements
        let total_elements = output.data.len() as f32;
        let normalized_gradient = &gradient / total_elements;

        Tensor { data: normalized_gradient, device: Device::default() }
    }
}

#[derive(Debug)]
pub struct MeanAbsoluteError;

impl Default for MeanAbsoluteError {
    fn default() -> Self {
        Self::new()
    }
}

impl MeanAbsoluteError {
    pub fn new() -> Self {
        Self
    }
}

impl Loss for MeanAbsoluteError {
    /// Calculates the mean absolute error (MAE) between two tensors.
    ///
    /// # Arguments
    ///
    /// * `y_true` - The ground truth tensor.
    /// * `y_pred` - The predicted tensor.
    ///
    /// # Returns
    ///
    /// The mean absolute error between the two tensors.
    fn calculate_loss(&self, y_true: &Tensor, y_pred: &Tensor) -> f32 {
        // Step 1: Ensure the shapes of y_true and y_pred match
        if y_true.data.shape() != y_pred.data.shape() {
            panic!(
                "Shape mismatch: y_true.shape = {:?}, y_pred.shape = {:?}",
                y_true.data.shape(),
                y_pred.data.shape()
            );
        }

        // Step 2: Check for NaN values in y_true and y_pred
        check_for_nan(y_true, y_pred);

        // Step 3: Compute the absolute differences
        let abs_diff = (&y_true.data - &y_pred.data).mapv(|x| x.abs());

        // Step 4: Calculate the mean of the absolute differences
        if abs_diff.is_empty() {
            panic!("Cannot calculate loss: no dataset in input tensors");
        }

        abs_diff.mean().expect("Mean computation failed unexpectedly")
    }

    /// Calculates the gradient of the loss with respect to the output tensor.
    ///
    /// # Arguments
    ///
    /// * `output` - The output tensor from the model.
    /// * `target` - The target tensor.
    ///
    /// # Returns
    ///
    /// A `Tensor` containing the gradient of the loss with respect to the output tensor.
    fn calculate_loss_grad(&self, output: &Tensor, target: &Tensor) -> Tensor {
        // Ensure shapes match
        if output.data.shape() != target.data.shape() {
            panic!(
                "Shape mismatch: output.shape = {:?}, target.shape = {:?}",
                output.data.shape(),
                target.data.shape()
            );
        }

        // Compute the gradient
        let diff = &output.data - &target.data;
        let gradient = diff.mapv(|x| if x > 0.0 { 1.0 } else { -1.0 });

        Tensor { data: gradient, device: Device::default() }
    }
}

/// A struct representing the Mean Squared Loss function.
#[derive(Debug)]
pub struct MeanSquaredLoss;

impl Default for MeanSquaredLoss {
    fn default() -> Self {
        Self::new()
    }
}

impl MeanSquaredLoss {
    /// Creates a new instance of `MeanSquaredLoss`.
    pub fn new() -> Self {
        Self
    }
}

impl Loss for MeanSquaredLoss {
    /// Calculates the mean squared error (MSE) between two tensors.
    ///
    /// # Arguments
    ///
    /// * `y_true` - The ground truth tensor.
    /// * `y_pred` - The predicted tensor.
    ///
    /// # Returns
    ///
    /// The mean squared error between the two tensors.
    fn calculate_loss(&self, y_true: &Tensor, y_pred: &Tensor) -> f32 {
        // Step 1: Ensure the shapes of y_true and y_pred match
        if y_true.data.shape() != y_pred.data.shape() {
            panic!(
                "Shape mismatch: y_true.shape = {:?}, y_pred.shape = {:?}",
                y_true.data.shape(),
                y_pred.data.shape()
            );
        }

        // Step 2: Check for NaN values in y_true and y_pred
        check_for_nan(y_true, y_pred);

        // Step 3: Compute the squared differences
        let squared_diff = (&y_true.data - &y_pred.data).mapv(|x| x.powi(2));

        // Step 4: Calculate the mean of the squared differences
        if squared_diff.is_empty() {
            panic!("Cannot calculate loss: no dataset in input tensors");
        }

        squared_diff.mean().expect("Mean computation failed unexpectedly")
    }

    /// Calculates the gradient of the loss with respect to the output tensor.
    ///
    /// # Arguments
    ///
    /// * `output` - The output tensor from the model.
    /// * `target` - The target tensor.
    ///
    /// # Returns
    ///
    /// A `Tensor` containing the gradient of the loss with respect to the output tensor.
    fn calculate_loss_grad(&self, output: &Tensor, target: &Tensor) -> Tensor {
        // Ensure shapes match
        if output.data.shape() != target.data.shape() {
            panic!(
                "Shape mismatch: output.shape = {:?}, target.shape = {:?}",
                output.data.shape(),
                target.data.shape()
            );
        }

        // Calculate the total number of elements in the tensor
        let total_elements = output.data.len() as f32;

        // Compute the gradient
        let diff = &output.data - &target.data;
        let gradient = &diff * 2.0 / total_elements;

        Tensor { data: gradient, device: Device::default() }
    }
}

/// A struct representing the Sparse Categorical Cross-Entropy Loss function.
#[derive(Debug)]
pub struct SparseCategoricalCrossEntropyLoss;

impl Default for SparseCategoricalCrossEntropyLoss {
    fn default() -> Self {
        Self::new()
    }
}

impl SparseCategoricalCrossEntropyLoss {
    /// Creates a new SparseCategoricalCrossEntropyLoss instance.
    pub fn new() -> Self {
        Self
    }

    /// Clips the tensor to avoid issues with log(0) or division by zero.
    ///
    /// # Arguments
    ///
    /// * `tensor` - The tensor to clip.
    /// * `epsilon` - The epsilon value to use for clipping.
    ///
    /// # Returns
    ///
    /// The clipped tensor.
    fn clip_tensor(&self, tensor: &Tensor, epsilon: f32) -> Tensor {
        tensor.map(|x| x.clamp(epsilon, 1.0 - epsilon))
    }

    /// Converts one-hot encoded `y_true` to class indices.
    ///
    /// # Arguments
    ///
    /// * `one_hot` - The one-hot encoded tensor.
    ///
    /// # Returns
    ///
    /// A tensor containing class indices.
    fn one_hot_to_indices(&self, one_hot: &Tensor) -> Tensor {
        if one_hot.shape().raw_dim().ndim() != 2 {
            panic!(
                "Expected a 2D tensor for one-hot encoding, but got shape: {:?}",
                one_hot.shape()
            );
        }

        let rows = one_hot.shape().raw_dim()[0];
        let indices: Vec<f32> = one_hot
            .data
            .outer_iter()
            .map(|row| {
                row.iter()
                    .position(|&x| x == 1.0)
                    .expect("One-hot encoding must have exactly one '1' per row")
                    as f32
            })
            .collect();

        let mut indices = Tensor::new(indices, Shape::from(IxDyn(&[rows])));
        indices.device = one_hot.device.clone();
        indices
    }

    /// Preprocesses one-hot encoded `y_true` for use with sparse categorical cross-entropy loss.
    ///
    /// # Arguments
    ///
    /// * `y_true` - The one-hot encoded tensor.
    ///
    /// # Returns
    ///
    /// A tensor containing class indices.
    fn preprocess_one_hot(&self, y_true: &Tensor) -> Tensor {
        let shape = y_true.shape();
        if shape.raw_dim().ndim() != 2 {
            panic!("Expected a 2D tensor for one-hot encoding, but got shape: {:?}", shape);
        }

        let rows = shape.raw_dim()[0];
        let cols = shape.raw_dim()[1];
        let data = y_true.data.as_slice().expect("Tensor dataset must be accessible as a slice");

        let mut processed_data = Vec::new();
        for i in 0..rows {
            let start = i * cols;
            let end = start + cols;
            let row = &data[start..end]; // Slice the row
            let max_idx = row
                .iter()
                .enumerate()
                .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                .map(|(idx, _)| idx)
                .expect("Row cannot be empty");
            processed_data.extend((0..cols).map(|j| if j == max_idx { 1.0 } else { 0.0 }));
        }

        let mut new_tensor = Tensor::new(processed_data, Shape::from(IxDyn(&[rows, cols])));
        new_tensor.device = y_true.device.clone();
        new_tensor
    }
}

impl Loss for SparseCategoricalCrossEntropyLoss {
    /// Calculates the sparse categorical cross-entropy loss between the true and predicted values.
    ///
    /// # Arguments
    ///
    /// * `y_true` - The true values (indices of the correct classes or one-hot encoded).
    /// * `y_pred` - The predicted values (probability distributions).
    ///
    /// # Returns
    ///
    /// The sparse categorical cross-entropy loss.
    fn calculate_loss(&self, y_true: &Tensor, y_pred: &Tensor) -> f32 {
        let y_true = if y_true.shape().raw_dim().ndim() == 2 {
            // Handle one-hot encoding
            if y_true.shape().raw_dim()[1] != y_pred.shape().raw_dim()[1] {
                panic!(
                    "If y_true is one-hot encoded, it must have the same number of classes as y_pred. \nGot y_true: {:?}\nGot y_pred: {:?}",
                    y_true.shape(),
                    y_pred.shape()
                );
            }
            let y_true = self.preprocess_one_hot(y_true);

            self.one_hot_to_indices(&y_true)
        } else {
            // Validate 1D tensor
            if y_true.shape().raw_dim().ndim() != 1 {
                panic!(
                    "Expected y_true to be a 1D tensor of class indices, but got shape: {:?}",
                    y_true.shape()
                );
            }
            y_true.clone()
        };

        // Check for NaN values in y_true and y_pred
        check_for_nan(&y_true, y_pred);

        // Clip predictions to avoid log(0)
        let epsilon = 1e-12;
        let clipped_pred = self.clip_tensor(y_pred, epsilon);

        // Compute cross-entropy loss
        let batch_size = y_true.shape().raw_dim()[0];
        let num_classes = y_pred.shape().raw_dim()[1];
        let mut loss = 0.0;

        for i in 0..batch_size {
            let true_class = y_true.data[i] as usize;
            if true_class >= num_classes {
                panic!(
                    "Invalid class index in y_true: {}, exceeds number of classes: {}",
                    true_class, num_classes
                );
            }
            loss -= clipped_pred.data[[i, true_class]].ln();
        }

        loss / batch_size as f32
    }

    /// Calculates the gradient of the sparse categorical cross-entropy loss with respect to the predicted values.
    ///
    /// # Arguments
    ///
    /// * `output` - The predicted values (probability distributions).
    /// * `target` - The true values (indices of the correct classes).
    ///
    /// # Returns
    ///
    /// The gradient of the sparse categorical cross-entropy loss with respect to the predicted values.
    fn calculate_loss_grad(&self, output: &Tensor, target: &Tensor) -> Tensor {
        let target = if target.shape().raw_dim().ndim() == 2 {
            // Handle one-hot encoding
            if target.shape().raw_dim()[1] != output.shape().raw_dim()[1] {
                panic!(
                    "If target is one-hot encoded, it must have the same number of classes as output. Got: {:?}",
                    target.shape()
                );
            }
            let target = self.preprocess_one_hot(target);

            self.one_hot_to_indices(&target)
        } else {
            // Validate 1D tensor
            if target.shape().raw_dim().ndim() != 1 {
                panic!(
                    "Expected target to be a 1D tensor of class indices, but got shape: {:?}",
                    target.shape()
                );
            }
            target.clone()
        };

        let batch_size = target.shape().raw_dim()[0];
        let num_classes = output.shape().raw_dim()[1];
        let epsilon = 1e-12;
        let clipped_pred = self.clip_tensor(output, epsilon);

        let mut grad_data = vec![0.0; clipped_pred.data.len()];
        for i in 0..batch_size {
            let true_class = target.data[i] as usize;
            if true_class >= num_classes {
                panic!(
                    "Invalid class index in target: {}, exceeds number of classes: {}",
                    true_class, num_classes
                );
            }
            for j in 0..num_classes {
                let pred = clipped_pred.data[[i, j]];
                grad_data[i * num_classes + j] = if j == true_class {
                    (pred - 1.0) / batch_size as f32
                } else {
                    pred / batch_size as f32
                };
            }
        }

        let grad_shape = clipped_pred.shape().raw_dim().as_array_view().to_vec();
        Tensor {
            data: ndarray::Array::from_shape_vec(ndarray::IxDyn(&grad_shape), grad_data)
                .expect("Failed to create gradient tensor"),
            device: Device::default(),
        }
    }
}

#[cfg(test)]
mod tests {
    use std::vec;

    use ndarray::{IxDyn, Shape};

    use super::*;

    #[test]
    fn test_cross_entropy_loss() {
        let y_true = Tensor::new(vec![1.0, 0.0, 0.0], Shape::from(IxDyn(&[1, 3])));
        let y_pred = Tensor::new(vec![0.7, 0.2, 0.1], Shape::from(IxDyn(&[1, 3])));

        let loss = CrossEntropyLoss::new();
        let calculated_loss = loss.calculate_loss(&y_true, &y_pred);

        let expected_loss = -(1.0 * (0.7f32.ln())) / 1.0;

        assert!(
            (calculated_loss - expected_loss).abs() < 1e-6,
            "Expected loss to be {:.6}, got {:.6}",
            expected_loss,
            calculated_loss
        );
    }

    #[test]
    fn test_cross_entropy_loss_grad() {
        let y_true = Tensor::new(vec![1.0, 0.0, 0.0], Shape::from(IxDyn(&[1, 3])));
        let y_pred = Tensor::new(vec![0.7, 0.2, 0.1], Shape::from(IxDyn(&[1, 3])));

        let loss = CrossEntropyLoss::new();
        let grad = loss.calculate_loss_grad(&y_pred, &y_true);

        let expected_grad = vec![-0.1, 0.06666667, 0.03333334]; // Adjusted to correct gradient
        assert!(
            grad.data.iter().zip(expected_grad.iter()).all(|(a, b)| (a - b).abs() < 1e-6),
            "Expected gradient: {:?}, got: {:?}",
            expected_grad,
            grad.data
        );
    }

    #[test]
    fn test_huber_loss() {
        let y_true = Tensor::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], Shape::from(IxDyn(&[2, 3])));
        let y_pred = Tensor::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], Shape::from(IxDyn(&[2, 3])));
        let loss = HuberLoss::new(1.0);
        let result = loss.calculate_loss(&y_true, &y_pred);
        assert_eq!(result, 0.0);
    }

    #[test]
    fn test_huber_loss_with_actual_values() {
        let y_true = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], Shape::from(IxDyn(&[2, 2])));
        let y_pred = Tensor::new(vec![2.5, 2.0, 3.0, 3.0], Shape::from(IxDyn(&[2, 2])));
        let loss = HuberLoss::new(1.0);
        let result = loss.calculate_loss(&y_true, &y_pred);

        let expected_loss = 0.375; // Updated to match actual calculation

        println!("Expected Loss: {}", expected_loss);
        println!("Calculated Loss: {}", result);

        assert!((result - expected_loss).abs() < 1e-6, "Calculated loss did not match expected");
    }

    #[test]
    fn test_huber_loss_with_nan() {
        let y_true = Tensor::new(vec![1.0, 2.0, f32::NAN, 4.0], Shape::from(IxDyn(&[2, 2])));
        let y_pred = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], Shape::from(IxDyn(&[2, 2])));
        let loss = HuberLoss::new(1.0);

        let result = std::panic::catch_unwind(|| {
            loss.calculate_loss(&y_true, &y_pred);
        });

        assert!(result.is_err(), "Expected a panic due to NaN in inputs, but no panic occurred.");
    }

    #[test]
    fn test_huber_loss_with_mismatch() {
        let y_true = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], Shape::from(IxDyn(&[2, 2])));
        let y_pred = Tensor::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], Shape::from(IxDyn(&[2, 3])));
        let loss = HuberLoss::new(1.0);

        let result = std::panic::catch_unwind(|| {
            loss.calculate_loss(&y_true, &y_pred);
        });

        assert!(result.is_err(), "Expected a panic due to shape mismatch, but no panic occurred.");
    }

    #[test]
    fn test_mean_absolute_error() {
        let y_true = Tensor::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], Shape::from(IxDyn(&[2, 3])));
        let y_pred = Tensor::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], Shape::from(IxDyn(&[2, 3])));
        let loss = MeanAbsoluteError::new();
        let result = loss.calculate_loss(&y_true, &y_pred);
        assert_eq!(result, 0.0);
    }

    #[test]
    fn test_mean_absolute_error_with_mismatch() {
        let y_true = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], Shape::from(IxDyn(&[2, 2])));
        let y_pred = Tensor::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], Shape::from(IxDyn(&[2, 3])));
        let loss = MeanAbsoluteError::new();

        let result = std::panic::catch_unwind(|| {
            loss.calculate_loss(&y_true, &y_pred);
        });

        assert!(result.is_err(), "Expected a panic due to shape mismatch, but no panic occurred.");
    }

    #[test]
    fn test_mean_absolute_error_with_nan() {
        let y_true = Tensor::new(vec![1.0, 2.0, f32::NAN, 4.0], Shape::from(IxDyn(&[2, 2])));
        let y_pred = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], Shape::from(IxDyn(&[2, 2])));
        let loss = MeanAbsoluteError::new();

        let result = std::panic::catch_unwind(|| {
            loss.calculate_loss(&y_true, &y_pred);
        });

        assert!(result.is_err(), "Expected a panic due to NaN in inputs, but no panic occurred.");
    }

    #[test]
    fn test_mean_absolute_error_with_actual_values() {
        let y_true = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], Shape::from(IxDyn(&[2, 2])));
        let y_pred = Tensor::new(vec![2.0, 3.0, 4.0, 5.0], Shape::from(IxDyn(&[2, 2])));
        let loss = MeanAbsoluteError::new();
        let result = loss.calculate_loss(&y_true, &y_pred);

        assert!(
            (result - 1.0).abs() < 1e-6,
            "Expected mean absolute error to be 1.0, got {}",
            result
        );
    }

    #[test]
    fn test_mean_squared_loss() {
        let y_true = Tensor::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], Shape::from(IxDyn(&[2, 3])));
        let y_pred = Tensor::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], Shape::from(IxDyn(&[2, 3])));
        let loss = MeanSquaredLoss::new();
        let result = loss.calculate_loss(&y_true, &y_pred);
        assert_eq!(result, 0.0);
    }

    #[test]
    fn test_mean_squared_loss_with_mismatch() {
        let y_true = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], Shape::from(IxDyn(&[2, 2])));
        let y_pred = Tensor::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], Shape::from(IxDyn(&[2, 3])));
        let loss = MeanSquaredLoss::new();

        let result = std::panic::catch_unwind(|| {
            loss.calculate_loss(&y_true, &y_pred);
        });

        assert!(result.is_err(), "Expected a panic due to shape mismatch, but no panic occurred.");
    }

    #[test]
    fn test_mean_squared_loss_with_nan() {
        let y_true = Tensor::new(vec![1.0, 2.0, f32::NAN, 4.0], Shape::from(IxDyn(&[2, 2])));
        let y_pred = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], Shape::from(IxDyn(&[2, 2])));
        let loss = MeanSquaredLoss::new();

        let result = std::panic::catch_unwind(|| {
            loss.calculate_loss(&y_true, &y_pred);
        });

        assert!(result.is_err(), "Expected a panic due to NaN in inputs, but no panic occurred.");
    }

    #[test]
    fn test_mean_squared_loss_with_actual_values() {
        let y_true = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], Shape::from(IxDyn(&[2, 2])));
        let y_pred = Tensor::new(vec![2.0, 3.0, 4.0, 5.0], Shape::from(IxDyn(&[2, 2])));
        let loss = MeanSquaredLoss::new();
        let result = loss.calculate_loss(&y_true, &y_pred);

        assert!(
            (result - 1.0).abs() < 1e-6,
            "Expected mean squared loss to be 1.0, got {}",
            result
        );
    }

    #[test]
    fn test_sparse_categorical_cross_entropy_loss() {
        let y_true = Tensor::new(vec![0.0, 1.0, 2.0], Shape::from(IxDyn(&[3])));
        let y_pred = Tensor::new(
            vec![0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
            Shape::from(IxDyn(&[3, 3])),
        );
        let loss = SparseCategoricalCrossEntropyLoss::new();
        let loss_value = loss.calculate_loss(&y_true, &y_pred);
        assert_eq!(loss_value, 1.0336976);
    }

    #[test]
    fn test_sparse_categorical_cross_entropy_loss_grad() {
        let y_true = Tensor::new(vec![0.0, 1.0, 2.0], Shape::from(IxDyn(&[3])));
        let y_pred = Tensor::new(
            vec![0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
            Shape::from(IxDyn(&[3, 3])),
        );
        let loss = SparseCategoricalCrossEntropyLoss::new();
        let grad = loss.calculate_loss_grad(&y_pred, &y_true);
        assert_eq!(grad.data.shape(), &[3, 3]);
    }

    #[test]
    fn test_preprocess_one_hot() {
        let y_true = Tensor::new(vec![1.0, 0.0, 0.0], Shape::from(IxDyn(&[1, 3])));
        let loss = SparseCategoricalCrossEntropyLoss::new();
        let result = loss.preprocess_one_hot(&y_true);
        assert_eq!(result.data.iter().cloned().collect::<Vec<f32>>(), vec![1.0, 0.0, 0.0]);
    }
}
